{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyP4jgVO6dpn7VwlxWDqtmBc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"TwCeHHkUicAO"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import os\n","import gc\n","\n","from sklearn.model_selection import train_test_split\n","\n","\n","import tensorflow as tf\n","from tqdm.autonotebook import tqdm\n","\n","import numpy as np\n","import pandas as pd\n","\n","from keras import Sequential\n","from keras.callbacks import EarlyStopping\n","\n","from keras.optimizers import Adam, SGD\n","from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n","from keras.layers import Flatten,Dense,BatchNormalization,Activation,Dropout\n","from keras.layers import Lambda, Input, GlobalAveragePooling2D,BatchNormalization\n","from keras.utils import to_categorical\n","\n","from tensorflow.keras.models import Model\n","\n","\n","from keras.preprocessing.image import load_img\n","from google.colab import drive\n","drive.mount('/content/drive')\n","model_path = '/content/drive/My Drive/notebook_data_and_output/model2/DogClassification.h5'\n","print(\"GPU\", \"available (Yes)\" if tf.config.list_physical_devices(\"GPU\") else \"not available :(\")\n","tf.config.list_physical_devices(\"GPU\")\n","print(os.listdir('/content/drive/My Drive/notebook_data_and_output/dataset2/'))\n","labels = pd.read_csv('/content/drive/My Drive/notebook_data_and_output/dataset2/labels.csv')\n","labels.head()\n","labels.describe()\n","def barw(ax):\n","\n","    for p in ax.patches:\n","        val = p.get_width() #height of the bar\n","        x = p.get_x()+ p.get_width() # x- position\n","        y = p.get_y() + p.get_height()/2 #y-position\n","        ax.annotate(round(val,2),(x,y))\n","\n","plt.figure(figsize = (15,30))\n","ax0 =sns.countplot(y=labels['breed'],order=labels['breed'].value_counts().index)\n","barw(ax0)\n","plt.show()\n","import os\n","print('Number of file matches number of actual images' if len(os.listdir('/content/drive/My Drive/notebook_data_and_output/dataset2/train/')) == len(labels['id']) else 'Number of file does not match number of actual images')\n","classes = sorted(list(set(labels['breed'])))\n","n_classes = len(classes)\n","print('Total unique breed {}'.format(n_classes))\n","class_to_num = dict(zip(classes, range(n_classes)))\n","class_to_num\n","input_shape = (331,331,3)\n","\n","\n","def images_to_array(directory, label_dataframe, target_size = input_shape):\n","\n","    image_labels = label_dataframe['breed']\n","    images = np.zeros([len(label_dataframe), target_size[0], target_size[1], target_size[2]],dtype=np.uint8) #as we have huge data and limited ram memory. uint8 takes less memory\n","    y = np.zeros([len(label_dataframe),1],dtype = np.uint8)\n","\n","    for ix, image_name in enumerate(tqdm(label_dataframe['id'].values)):\n","        img_dir = os.path.join(directory, image_name + '.jpg')\n","        img = load_img(img_dir, target_size = target_size)\n","#         img = np.expand_dims(img, axis=0)\n","#         img = processed_image_resnet(img)\n","#         img = img/255\n","        images[ix]=img\n","#         images[ix] = img_to_array(img)\n","        del img\n","\n","        dog_breed = image_labels[ix]\n","        y[ix] = class_to_num[dog_breed]\n","\n","    y = to_categorical(y)\n","\n","    return images,y\n","    import time\n","t = time.time()\n","\n","X,y = images_to_array('/content/drive/My Drive/notebook_data_and_output/dataset2/train/', labels[:])\n","\n","print('runtime in seconds: {}'.format(time.time() - t))\n","# np.where(y[5]==1)[0][0]\n","\n","n=25\n","\n","plt.figure(figsize=(20,20))\n","\n","for i in range(n):\n","#     print(i)\n","    ax = plt.subplot(5, 5, i+1)\n","    plt.title(classes[np.where(y[i] ==1)[0][0]])\n","    plt.imshow(X[i].astype('int32'))\n","lrr= ReduceLROnPlateau(monitor='val_acc', factor=.01, patience=3, min_lr=1e-5,verbose = 1)\n","EarlyStop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","model_checkpoint = ModelCheckpoint(filepath=model_path, save_best_only=True, save_weights_only=True, monitor='val_loss', mode='min', verbose=1)\n","batch_size= 128\n","epochs=50\n","learn_rate=.001\n","sgd=SGD(lr=learn_rate,momentum=.9,nesterov=False)\n","adam=Adam(lr=learn_rate, beta_1=0.9, beta_2=0.999, epsilon=None,  amsgrad=False)\n","img_size = (331,331,3)\n","\n","def get_features(model_name, model_preprocessor, input_size, data):\n","\n","    input_layer = Input(input_size)\n","    preprocessor = Lambda(model_preprocessor)(input_layer)\n","    base_model = model_name(weights='imagenet', include_top=False,\n","                            input_shape=input_size)(preprocessor)\n","    avg = GlobalAveragePooling2D()(base_model)\n","    feature_extractor = Model(inputs = input_layer, outputs = avg)\n","\n","    #Extract feature.\n","    feature_maps = feature_extractor.predict(data, verbose=1)\n","    print('Feature maps shape: ', feature_maps.shape)\n","    return feature_maps\n","from keras.applications.inception_v3 import InceptionV3, preprocess_input\n","inception_preprocessor = preprocess_input\n","inception_features = get_features(InceptionV3,\n","                                  inception_preprocessor,\n","                                  img_size, X)\n","from keras.applications.xception import Xception, preprocess_input\n","xception_preprocessor = preprocess_input\n","xception_features = get_features(Xception,\n","                                 xception_preprocessor,\n","                                 img_size, X)\n","from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n","inc_resnet_preprocessor = preprocess_input\n","inc_resnet_features = get_features(InceptionResNetV2,\n","                                   inc_resnet_preprocessor,\n","                                   img_size, X)\n","from keras.applications.nasnet import NASNetLarge, preprocess_input\n","nasnet_preprocessor = preprocess_input\n","nasnet_features = get_features(NASNetLarge,\n","                               nasnet_preprocessor,\n","                               img_size, X)\n","del X\n","gc.collect()\n","final_features = np.concatenate([inception_features,\n","                                 xception_features,\n","                                 nasnet_features,\n","                                 inc_resnet_features,], axis=-1) #axis=-1 to concatinate horizontally\n","\n","print('Final feature maps shape', final_features.shape)\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","import matplotlib.pyplot as plt\n","\n","# Assuming final_features and y are defined properly\n","# Define your model\n","model = Sequential()\n","model.add(Dropout(0.7, input_shape=(final_features.shape[1],)))\n","model.add(Dense(n_classes, activation='softmax'))\n","\n","# Compile the model with the optimizer\n","adam = Adam(learning_rate=0.001)\n","model.compile(optimizer=adam,\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Print model summary to check the architecture\n","model.summary()\n","\n","# Train the model\n","history = model.fit(final_features, y,\n","                    batch_size=batch_size,\n","                    epochs=epochs,\n","                    validation_split=0.2,\n","                    callbacks=[lrr, EarlyStop, model_checkpoint],\n","                    verbose=1)\n","# Save the model\n","model.save(model_path)\n","# Plotting the training history\n","train_loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","epochs_range = range(1, len(train_loss) + 1)\n","\n","plt.figure(figsize=(10, 6))\n","plt.plot(epochs_range, train_loss, label='Training Loss')\n","plt.plot(epochs_range, val_loss, label='Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title('Training and Validation Loss')\n","plt.legend()\n","plt.show()\n","del inception_features\n","del xception_features\n","del nasnet_features\n","del inc_resnet_features\n","del final_features\n","gc.collect()\n","def images_to_array_test(test_path, img_size = (331,331,3)):\n","    test_filenames = [test_path + fname for fname in os.listdir(test_path)]\n","\n","    data_size = len(test_filenames)\n","    images = np.zeros([data_size, img_size[0], img_size[1], 3], dtype=np.uint8)\n","\n","\n","    for ix,img_dir in enumerate(tqdm(test_filenames)):\n","#         img_dir = os.path.join(directory, image_name + '.jpg')\n","        img = load_img(img_dir, target_size = img_size)\n","#         img = np.expand_dims(img, axis=0)\n","#         img = processed_image_resnet(img)\n","#         img = img/255\n","        images[ix]=img\n","#         images[ix] = img_to_array(img)\n","        del img\n","    print('Ouptut Data Size: ', images.shape)\n","    return images\n","\n","test_data = images_to_array_test('/content/drive/My Drive/notebook_data_and_output/dataset2/test/', img_size)\n","def extact_features(data):\n","    inception_features = get_features(InceptionV3, inception_preprocessor, img_size, data)\n","    xception_features = get_features(Xception, xception_preprocessor, img_size, data)\n","    nasnet_features = get_features(NASNetLarge, nasnet_preprocessor, img_size, data)\n","    inc_resnet_features = get_features(InceptionResNetV2, inc_resnet_preprocessor, img_size, data)\n","\n","    final_features = np.concatenate([inception_features,\n","                                     xception_features,\n","                                     nasnet_features,\n","                                     inc_resnet_features],axis=-1)\n","\n","    print('Final feature maps shape', final_features.shape)\n","\n","    #deleting to free up ram memory\n","    del inception_features\n","    del xception_features\n","    del nasnet_features\n","    del inc_resnet_features\n","    gc.collect()\n","\n","\n","    return final_features\n","\n","test_features = extact_features(test_data)\n","del test_data\n","gc.collect()\n","pred = model.predict(test_features)\n","print(pred[0])\n","print(f\"Max value (probability of prediction): {np.max(pred[0])}\") # the max probability value predicted by the model\n","print(f\"Sum: {np.sum(pred[0])}\") # because we used softmax activation in our model, this will be close to 1\n","print(f\"Max index: {np.argmax(pred[0])}\") # the index of where the max value in predictions[0] occurs\n","print(f\"Predicted label: {classes[np.argmax(pred[0])]}\")\n","preds_df = pd.DataFrame(columns=[\"id\"] + list(classes))\n","preds_df.head()\n","test_path = \"/content/drive/My Drive/notebook_data_and_output/dataset2/test/\"\n","preds_df[\"id\"] = [os.path.splitext(path)[0] for path in os.listdir(test_path)]\n","preds_df.head()\n","preds_df.loc[:,list(classes)]= pred\n","\n","preds_df.to_csv('sample_submission.csv',index=None)\n","preds_df.head()"]}]}